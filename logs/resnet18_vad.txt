python train.py --nepoch 50 --model-mode=3 --save-name='./models/resnet18_vad.h5' --batch=8196
2022-02-05 14:27:58.109597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-05 14:27:58.147271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-05 14:27:58.149121: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
data shape is (131118, 65, 16, 1)
2022-02-05 14:27:58.957756: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-02-05 14:27:58.963853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-05 14:27:58.965622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-05 14:27:58.967337: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-05 14:27:59.805379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-05 14:27:59.807231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-05 14:27:59.808839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-02-05 14:27:59.810355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46710 MB memory:  -> device: 0, name: RTX A6000, pci bus id: 0000:11:00.0, compute capability: 8.6
Model: "res_net2d18"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            [(None, 65, 16, 1)]  0
__________________________________________________________________________________________________
conv1 (Conv2D)                  (None, 33, 8, 64)    3136        input_1[0][0]
__________________________________________________________________________________________________
bn_conv1 (BatchNormalization)   (None, 33, 8, 64)    256         conv1[0][0]
__________________________________________________________________________________________________
conv1_relu (Activation)         (None, 33, 8, 64)    0           bn_conv1[0][0]
__________________________________________________________________________________________________
pool1 (MaxPooling2D)            (None, 17, 4, 64)    0           conv1_relu[0][0]
__________________________________________________________________________________________________
padding2a_branch2a (ZeroPadding (None, 19, 6, 64)    0           pool1[0][0]
__________________________________________________________________________________________________
res2a_branch2a (Conv2D)         (None, 17, 4, 64)    36864       padding2a_branch2a[0][0]
__________________________________________________________________________________________________
bn2a_branch2a (BatchNormalizati (None, 17, 4, 64)    256         res2a_branch2a[0][0]
__________________________________________________________________________________________________
res2a_branch2a_relu (Activation (None, 17, 4, 64)    0           bn2a_branch2a[0][0]
__________________________________________________________________________________________________
padding2a_branch2b (ZeroPadding (None, 19, 6, 64)    0           res2a_branch2a_relu[0][0]
__________________________________________________________________________________________________
res2a_branch2b (Conv2D)         (None, 17, 4, 64)    36864       padding2a_branch2b[0][0]
__________________________________________________________________________________________________
res2a_branch1 (Conv2D)          (None, 17, 4, 64)    4096        pool1[0][0]
__________________________________________________________________________________________________
bn2a_branch2b (BatchNormalizati (None, 17, 4, 64)    256         res2a_branch2b[0][0]
__________________________________________________________________________________________________
bn2a_branch1 (BatchNormalizatio (None, 17, 4, 64)    256         res2a_branch1[0][0]
__________________________________________________________________________________________________
res2a (Add)                     (None, 17, 4, 64)    0           bn2a_branch2b[0][0]
                                                                 bn2a_branch1[0][0]
__________________________________________________________________________________________________
res2a_relu (Activation)         (None, 17, 4, 64)    0           res2a[0][0]
__________________________________________________________________________________________________
padding2b1_branch2a (ZeroPaddin (None, 19, 6, 64)    0           res2a_relu[0][0]
__________________________________________________________________________________________________
res2b1_branch2a (Conv2D)        (None, 17, 4, 64)    36864       padding2b1_branch2a[0][0]
__________________________________________________________________________________________________
bn2b1_branch2a (BatchNormalizat (None, 17, 4, 64)    256         res2b1_branch2a[0][0]
__________________________________________________________________________________________________
res2b1_branch2a_relu (Activatio (None, 17, 4, 64)    0           bn2b1_branch2a[0][0]
__________________________________________________________________________________________________
padding2b1_branch2b (ZeroPaddin (None, 19, 6, 64)    0           res2b1_branch2a_relu[0][0]
__________________________________________________________________________________________________
res2b1_branch2b (Conv2D)        (None, 17, 4, 64)    36864       padding2b1_branch2b[0][0]
__________________________________________________________________________________________________
bn2b1_branch2b (BatchNormalizat (None, 17, 4, 64)    256         res2b1_branch2b[0][0]
__________________________________________________________________________________________________
res2b1 (Add)                    (None, 17, 4, 64)    0           bn2b1_branch2b[0][0]
                                                                 res2a_relu[0][0]
__________________________________________________________________________________________________
res2b1_relu (Activation)        (None, 17, 4, 64)    0           res2b1[0][0]
__________________________________________________________________________________________________
padding3a_branch2a (ZeroPadding (None, 19, 6, 64)    0           res2b1_relu[0][0]
__________________________________________________________________________________________________
res3a_branch2a (Conv2D)         (None, 9, 2, 128)    73728       padding3a_branch2a[0][0]
__________________________________________________________________________________________________
bn3a_branch2a (BatchNormalizati (None, 9, 2, 128)    512         res3a_branch2a[0][0]
__________________________________________________________________________________________________
res3a_branch2a_relu (Activation (None, 9, 2, 128)    0           bn3a_branch2a[0][0]
__________________________________________________________________________________________________
padding3a_branch2b (ZeroPadding (None, 11, 4, 128)   0           res3a_branch2a_relu[0][0]
__________________________________________________________________________________________________
res3a_branch2b (Conv2D)         (None, 9, 2, 128)    147456      padding3a_branch2b[0][0]
__________________________________________________________________________________________________
res3a_branch1 (Conv2D)          (None, 9, 2, 128)    8192        res2b1_relu[0][0]
__________________________________________________________________________________________________
bn3a_branch2b (BatchNormalizati (None, 9, 2, 128)    512         res3a_branch2b[0][0]
__________________________________________________________________________________________________
bn3a_branch1 (BatchNormalizatio (None, 9, 2, 128)    512         res3a_branch1[0][0]
__________________________________________________________________________________________________
res3a (Add)                     (None, 9, 2, 128)    0           bn3a_branch2b[0][0]
                                                                 bn3a_branch1[0][0]
__________________________________________________________________________________________________
res3a_relu (Activation)         (None, 9, 2, 128)    0           res3a[0][0]
__________________________________________________________________________________________________
padding3b1_branch2a (ZeroPaddin (None, 11, 4, 128)   0           res3a_relu[0][0]
__________________________________________________________________________________________________
res3b1_branch2a (Conv2D)        (None, 9, 2, 128)    147456      padding3b1_branch2a[0][0]
__________________________________________________________________________________________________
bn3b1_branch2a (BatchNormalizat (None, 9, 2, 128)    512         res3b1_branch2a[0][0]
__________________________________________________________________________________________________
res3b1_branch2a_relu (Activatio (None, 9, 2, 128)    0           bn3b1_branch2a[0][0]
__________________________________________________________________________________________________
padding3b1_branch2b (ZeroPaddin (None, 11, 4, 128)   0           res3b1_branch2a_relu[0][0]
__________________________________________________________________________________________________
res3b1_branch2b (Conv2D)        (None, 9, 2, 128)    147456      padding3b1_branch2b[0][0]
__________________________________________________________________________________________________
bn3b1_branch2b (BatchNormalizat (None, 9, 2, 128)    512         res3b1_branch2b[0][0]
__________________________________________________________________________________________________
res3b1 (Add)                    (None, 9, 2, 128)    0           bn3b1_branch2b[0][0]
                                                                 res3a_relu[0][0]
__________________________________________________________________________________________________
res3b1_relu (Activation)        (None, 9, 2, 128)    0           res3b1[0][0]
__________________________________________________________________________________________________
padding4a_branch2a (ZeroPadding (None, 11, 4, 128)   0           res3b1_relu[0][0]
__________________________________________________________________________________________________
res4a_branch2a (Conv2D)         (None, 5, 1, 256)    294912      padding4a_branch2a[0][0]
__________________________________________________________________________________________________
bn4a_branch2a (BatchNormalizati (None, 5, 1, 256)    1024        res4a_branch2a[0][0]
__________________________________________________________________________________________________
res4a_branch2a_relu (Activation (None, 5, 1, 256)    0           bn4a_branch2a[0][0]
__________________________________________________________________________________________________
padding4a_branch2b (ZeroPadding (None, 7, 3, 256)    0           res4a_branch2a_relu[0][0]
__________________________________________________________________________________________________
res4a_branch2b (Conv2D)         (None, 5, 1, 256)    589824      padding4a_branch2b[0][0]
__________________________________________________________________________________________________
res4a_branch1 (Conv2D)          (None, 5, 1, 256)    32768       res3b1_relu[0][0]
__________________________________________________________________________________________________
bn4a_branch2b (BatchNormalizati (None, 5, 1, 256)    1024        res4a_branch2b[0][0]
__________________________________________________________________________________________________
bn4a_branch1 (BatchNormalizatio (None, 5, 1, 256)    1024        res4a_branch1[0][0]
__________________________________________________________________________________________________
res4a (Add)                     (None, 5, 1, 256)    0           bn4a_branch2b[0][0]
                                                                 bn4a_branch1[0][0]
__________________________________________________________________________________________________
res4a_relu (Activation)         (None, 5, 1, 256)    0           res4a[0][0]
__________________________________________________________________________________________________
padding4b1_branch2a (ZeroPaddin (None, 7, 3, 256)    0           res4a_relu[0][0]
__________________________________________________________________________________________________
res4b1_branch2a (Conv2D)        (None, 5, 1, 256)    589824      padding4b1_branch2a[0][0]
__________________________________________________________________________________________________
bn4b1_branch2a (BatchNormalizat (None, 5, 1, 256)    1024        res4b1_branch2a[0][0]
__________________________________________________________________________________________________
res4b1_branch2a_relu (Activatio (None, 5, 1, 256)    0           bn4b1_branch2a[0][0]
__________________________________________________________________________________________________
padding4b1_branch2b (ZeroPaddin (None, 7, 3, 256)    0           res4b1_branch2a_relu[0][0]
__________________________________________________________________________________________________
res4b1_branch2b (Conv2D)        (None, 5, 1, 256)    589824      padding4b1_branch2b[0][0]
__________________________________________________________________________________________________
bn4b1_branch2b (BatchNormalizat (None, 5, 1, 256)    1024        res4b1_branch2b[0][0]
__________________________________________________________________________________________________
res4b1 (Add)                    (None, 5, 1, 256)    0           bn4b1_branch2b[0][0]
                                                                 res4a_relu[0][0]
__________________________________________________________________________________________________
res4b1_relu (Activation)        (None, 5, 1, 256)    0           res4b1[0][0]
__________________________________________________________________________________________________
padding5a_branch2a (ZeroPadding (None, 7, 3, 256)    0           res4b1_relu[0][0]
__________________________________________________________________________________________________
res5a_branch2a (Conv2D)         (None, 3, 1, 512)    1179648     padding5a_branch2a[0][0]
__________________________________________________________________________________________________
bn5a_branch2a (BatchNormalizati (None, 3, 1, 512)    2048        res5a_branch2a[0][0]
__________________________________________________________________________________________________
res5a_branch2a_relu (Activation (None, 3, 1, 512)    0           bn5a_branch2a[0][0]
__________________________________________________________________________________________________
padding5a_branch2b (ZeroPadding (None, 5, 3, 512)    0           res5a_branch2a_relu[0][0]
__________________________________________________________________________________________________
res5a_branch2b (Conv2D)         (None, 3, 1, 512)    2359296     padding5a_branch2b[0][0]
__________________________________________________________________________________________________
res5a_branch1 (Conv2D)          (None, 3, 1, 512)    131072      res4b1_relu[0][0]
__________________________________________________________________________________________________
bn5a_branch2b (BatchNormalizati (None, 3, 1, 512)    2048        res5a_branch2b[0][0]
__________________________________________________________________________________________________
bn5a_branch1 (BatchNormalizatio (None, 3, 1, 512)    2048        res5a_branch1[0][0]
__________________________________________________________________________________________________
res5a (Add)                     (None, 3, 1, 512)    0           bn5a_branch2b[0][0]
                                                                 bn5a_branch1[0][0]
__________________________________________________________________________________________________
res5a_relu (Activation)         (None, 3, 1, 512)    0           res5a[0][0]
__________________________________________________________________________________________________
padding5b1_branch2a (ZeroPaddin (None, 5, 3, 512)    0           res5a_relu[0][0]
__________________________________________________________________________________________________
res5b1_branch2a (Conv2D)        (None, 3, 1, 512)    2359296     padding5b1_branch2a[0][0]
__________________________________________________________________________________________________
bn5b1_branch2a (BatchNormalizat (None, 3, 1, 512)    2048        res5b1_branch2a[0][0]
__________________________________________________________________________________________________
res5b1_branch2a_relu (Activatio (None, 3, 1, 512)    0           bn5b1_branch2a[0][0]
__________________________________________________________________________________________________
padding5b1_branch2b (ZeroPaddin (None, 5, 3, 512)    0           res5b1_branch2a_relu[0][0]
__________________________________________________________________________________________________
res5b1_branch2b (Conv2D)        (None, 3, 1, 512)    2359296     padding5b1_branch2b[0][0]
__________________________________________________________________________________________________
bn5b1_branch2b (BatchNormalizat (None, 3, 1, 512)    2048        res5b1_branch2b[0][0]
__________________________________________________________________________________________________
res5b1 (Add)                    (None, 3, 1, 512)    0           bn5b1_branch2b[0][0]
                                                                 res5a_relu[0][0]
__________________________________________________________________________________________________
res5b1_relu (Activation)        (None, 3, 1, 512)    0           res5b1[0][0]
__________________________________________________________________________________________________
pool5 (GlobalAveragePooling2D)  (None, 512)          0           res5b1_relu[0][0]
__________________________________________________________________________________________________
fc1000 (Dense)                  (None, 2)            1026        pool5[0][0]
==================================================================================================
Total params: 11,185,218
Trainable params: 11,175,490
Non-trainable params: 9,728
__________________________________________________________________________________________________
2022-02-05 14:28:02.428664: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/50
2022-02-05 14:28:05.394823: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201
2022-02-05 14:28:09.010787: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
12/12 [==============================] - 22s 614ms/step - loss: 0.3875 - accuracy: 0.8889 - val_loss: 92.9889 - val_accuracy: 0.1896
Epoch 2/50
12/12 [==============================] - 3s 283ms/step - loss: 0.1128 - accuracy: 0.9638 - val_loss: 48.4355 - val_accuracy: 0.1896
Epoch 3/50
12/12 [==============================] - 3s 280ms/step - loss: 0.0945 - accuracy: 0.9705 - val_loss: 30.8109 - val_accuracy: 0.1896
Epoch 4/50
12/12 [==============================] - 3s 280ms/step - loss: 0.0870 - accuracy: 0.9727 - val_loss: 19.6455 - val_accuracy: 0.1896
Epoch 5/50
12/12 [==============================] - 3s 284ms/step - loss: 0.0834 - accuracy: 0.9733 - val_loss: 10.7396 - val_accuracy: 0.1919
Epoch 6/50
12/12 [==============================] - 3s 282ms/step - loss: 0.0802 - accuracy: 0.9740 - val_loss: 6.3439 - val_accuracy: 0.2262
Epoch 7/50
12/12 [==============================] - 3s 284ms/step - loss: 0.0782 - accuracy: 0.9746 - val_loss: 3.4525 - val_accuracy: 0.3642
Epoch 8/50
12/12 [==============================] - 4s 305ms/step - loss: 0.0784 - accuracy: 0.9740 - val_loss: 1.8049 - val_accuracy: 0.5620
Epoch 9/50
12/12 [==============================] - 3s 281ms/step - loss: 0.0755 - accuracy: 0.9753 - val_loss: 1.2560 - val_accuracy: 0.6554
Epoch 10/50
12/12 [==============================] - 3s 282ms/step - loss: 0.0756 - accuracy: 0.9752 - val_loss: 1.0659 - val_accuracy: 0.7139
Epoch 11/50
12/12 [==============================] - 3s 282ms/step - loss: 0.0733 - accuracy: 0.9756 - val_loss: 0.2265 - val_accuracy: 0.9149
Epoch 12/50
12/12 [==============================] - 3s 281ms/step - loss: 0.0702 - accuracy: 0.9768 - val_loss: 0.1681 - val_accuracy: 0.9398
Epoch 13/50
12/12 [==============================] - 3s 282ms/step - loss: 0.0698 - accuracy: 0.9766 - val_loss: 0.1614 - val_accuracy: 0.9407
Epoch 14/50
12/12 [==============================] - 3s 283ms/step - loss: 0.0696 - accuracy: 0.9762 - val_loss: 0.1361 - val_accuracy: 0.9542
Epoch 15/50
12/12 [==============================] - 3s 283ms/step - loss: 0.0673 - accuracy: 0.9771 - val_loss: 0.1164 - val_accuracy: 0.9609
Epoch 16/50
12/12 [==============================] - 3s 284ms/step - loss: 0.0613 - accuracy: 0.9789 - val_loss: 0.1174 - val_accuracy: 0.9618
Epoch 17/50
12/12 [==============================] - 3s 283ms/step - loss: 0.0594 - accuracy: 0.9797 - val_loss: 0.1063 - val_accuracy: 0.9638
Epoch 18/50
12/12 [==============================] - 3s 285ms/step - loss: 0.0576 - accuracy: 0.9800 - val_loss: 0.1211 - val_accuracy: 0.9614
Epoch 19/50
12/12 [==============================] - 3s 285ms/step - loss: 0.0560 - accuracy: 0.9806 - val_loss: 0.1295 - val_accuracy: 0.9585
Epoch 20/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0557 - accuracy: 0.9807 - val_loss: 0.1538 - val_accuracy: 0.9386
Epoch 21/50
12/12 [==============================] - 3s 285ms/step - loss: 0.0659 - accuracy: 0.9769 - val_loss: 0.1742 - val_accuracy: 0.9411
Epoch 22/50
12/12 [==============================] - 3s 287ms/step - loss: 0.0600 - accuracy: 0.9794 - val_loss: 0.1098 - val_accuracy: 0.9656
Epoch 23/50
12/12 [==============================] - 3s 285ms/step - loss: 0.0556 - accuracy: 0.9804 - val_loss: 0.1282 - val_accuracy: 0.9594
Epoch 24/50
12/12 [==============================] - 3s 284ms/step - loss: 0.0513 - accuracy: 0.9816 - val_loss: 0.1542 - val_accuracy: 0.9504
Epoch 25/50
12/12 [==============================] - 3s 283ms/step - loss: 0.0457 - accuracy: 0.9838 - val_loss: 0.1568 - val_accuracy: 0.9496
Epoch 26/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0447 - accuracy: 0.9838 - val_loss: 0.1083 - val_accuracy: 0.9686
Epoch 27/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0482 - accuracy: 0.9830 - val_loss: 0.1555 - val_accuracy: 0.9519
Epoch 28/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0492 - accuracy: 0.9822 - val_loss: 0.1840 - val_accuracy: 0.9405
Epoch 29/50
12/12 [==============================] - 3s 285ms/step - loss: 0.0447 - accuracy: 0.9841 - val_loss: 0.1474 - val_accuracy: 0.9525
Epoch 30/50
12/12 [==============================] - 3s 285ms/step - loss: 0.0483 - accuracy: 0.9828 - val_loss: 0.1710 - val_accuracy: 0.9451
Epoch 31/50
12/12 [==============================] - 3s 287ms/step - loss: 0.0420 - accuracy: 0.9848 - val_loss: 0.1660 - val_accuracy: 0.9499
Epoch 32/50
12/12 [==============================] - 3s 287ms/step - loss: 0.0393 - accuracy: 0.9865 - val_loss: 0.1127 - val_accuracy: 0.9705
Epoch 33/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0347 - accuracy: 0.9879 - val_loss: 0.1898 - val_accuracy: 0.9385
Epoch 34/50
12/12 [==============================] - 3s 285ms/step - loss: 0.0363 - accuracy: 0.9872 - val_loss: 0.1838 - val_accuracy: 0.9599
Epoch 35/50
12/12 [==============================] - 3s 287ms/step - loss: 0.0504 - accuracy: 0.9817 - val_loss: 0.1254 - val_accuracy: 0.9679
Epoch 36/50
12/12 [==============================] - 3s 285ms/step - loss: 0.0424 - accuracy: 0.9850 - val_loss: 0.1742 - val_accuracy: 0.9483
Epoch 37/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0367 - accuracy: 0.9875 - val_loss: 0.1485 - val_accuracy: 0.9634
Epoch 38/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0346 - accuracy: 0.9881 - val_loss: 0.1733 - val_accuracy: 0.9531
Epoch 39/50
12/12 [==============================] - 3s 287ms/step - loss: 0.0322 - accuracy: 0.9889 - val_loss: 0.1322 - val_accuracy: 0.9659
Epoch 40/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0274 - accuracy: 0.9906 - val_loss: 0.1277 - val_accuracy: 0.9720
Epoch 41/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0275 - accuracy: 0.9906 - val_loss: 0.1734 - val_accuracy: 0.9571
Epoch 42/50
12/12 [==============================] - 3s 285ms/step - loss: 0.0260 - accuracy: 0.9914 - val_loss: 0.1314 - val_accuracy: 0.9700
Epoch 43/50
12/12 [==============================] - 3s 290ms/step - loss: 0.0256 - accuracy: 0.9915 - val_loss: 0.1840 - val_accuracy: 0.9578
Epoch 44/50
12/12 [==============================] - 3s 287ms/step - loss: 0.0262 - accuracy: 0.9911 - val_loss: 0.1568 - val_accuracy: 0.9599
Epoch 45/50
12/12 [==============================] - 3s 287ms/step - loss: 0.0243 - accuracy: 0.9914 - val_loss: 0.1584 - val_accuracy: 0.9638
Epoch 46/50
12/12 [==============================] - 3s 285ms/step - loss: 0.0341 - accuracy: 0.9883 - val_loss: 0.1579 - val_accuracy: 0.9627
Epoch 47/50
12/12 [==============================] - 3s 289ms/step - loss: 0.0286 - accuracy: 0.9903 - val_loss: 0.1903 - val_accuracy: 0.9561
Epoch 48/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0255 - accuracy: 0.9909 - val_loss: 0.1820 - val_accuracy: 0.9605
Epoch 49/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0268 - accuracy: 0.9908 - val_loss: 0.1394 - val_accuracy: 0.9702
Epoch 50/50
12/12 [==============================] - 3s 286ms/step - loss: 0.0207 - accuracy: 0.9929 - val_loss: 0.1613 - val_accuracy: 0.9674
[0.16067034006118774, 0.9654324650764465]
/home/gpufs/users/students/iasd22/iasd22_0904/miniconda3/envs/tf-gpu/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
  category=CustomMaskWarning)